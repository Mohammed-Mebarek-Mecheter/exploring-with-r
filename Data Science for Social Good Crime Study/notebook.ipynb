{"nbformat_minor":2,"metadata":{"language_info":{"file_extension":".r","name":"R","pygments_lexer":"r","mimetype":"text/x-r-source","version":"3.4.1","codemirror_mode":"r"},"kernelspec":{"language":"R","name":"ir","display_name":"R"}},"nbformat":4,"cells":[{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"3"},"editable":false},"cell_type":"markdown","source":"## 1.  The power of data science\n<p><img src=\"https://assets.datacamp.com/production/project_614/img/sf.jpg\" alt=\"San Francisco Skyline\"></p>\n<p>I believe a certain uncle once said to his benevolent nephew</p>\n<blockquote>\n  <p><em>With great power comes great responsibility</em></p>\n</blockquote>\n<p>This, of course, is an oft-quoted Spider-man line. I don't think any individual need bear the responsibility of a superhero, but it is important to understand the value of technical analysis skills in the context of modern social issues. Within the workplace, data science skills can greatly increase your utility as an employee or prospective applicant. What is not always  mentioned is that these same skills can be applied to positively impact your community. The concept of <strong>open science</strong> has led to new standards in <strong>open data</strong>, and there is an exciting plethora of raw information ready to be probed for insights. Organizations focused on <em>data science for social good</em> are rapidly growing, and the volunteer soup  kitchen seems to have a 21st-century rendition.</p>\n<p>Many local and federal governments support access to interesting datasets; as this trend grows, the utility of open information becomes more robust. Data is begging for audacious volunteers to poke and prod it, and make use of the raw input in an impactful way. We can now fight crime as a vigilante - all from behind a computer. </p>\n<p>In this notebook, we will explore San Francisco crime data in order to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends. </p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"3"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Load required packages\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Read in incidents dataset\nincidents <- read_csv(\"datasets/downsample_police-department-incidents.csv\")\n\n# Read in calls dataset\ncalls <- read_csv(\"datasets/downsample_police-department-calls-for-service.csv\")\n\nprint('Done!')"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"10"},"editable":false},"cell_type":"markdown","source":"## 2. First poke and prod\n<p>First things first: we need to wrap our heads around the data in order to understand <em>what</em> we have. Letâ€™s <code>glimpse()</code> the data to see if there are any variables in the two datasets that are the same or similar. Then we can ask an investigative question about these variables, and return a simple statistic such as a frequency count.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"10"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Glimpse the structure of both datasets\nglimpse(incidents)\nglimpse(calls)\n\n# Aggregate the number of reported incidents by Date\ndaily_incidents <- incidents %>%\n    count(Date, sort = TRUE) %>%\n    rename(n_incidents = n)\n\n# Aggregate the number of calls for police service by Date\ndaily_calls <- calls %>%\n    count(Date, sort = TRUE) %>%\n    rename(n_calls = n)"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"17"},"editable":false},"cell_type":"markdown","source":"## 3. Mutating join\n<p>Now that we have a better understanding of what variables are present in our information set we can see there are shared variables that will allow us to ask a wider variety of questions. We can inquire about the relationship between civilian-reported incidents and police-reported incidents by the date on which the incidents were documented. To combine this information we will perform a type of mutating join between the data frames. The new dataset structure preserves only days on which both civilians reported incidents and police encountered incidents.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"17"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Join data frames to create a new \"mutated\" set of information\nshared_dates <- inner_join(daily_incidents, daily_calls, by = \"Date\")\n\n# Take a glimpse of this new data frame\nglimpse(shared_dates)"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"24"},"editable":false},"cell_type":"markdown","source":"## 4. Inspect frequency trends\n<p>We now have a data frame that contains new information generated by combining datasets. In order to understand this new information we must visualize it. And I don't mean just giving the data a <code>glimpse()</code> - a table of raw information limits our comprehension of crime patterns. A picture is worth a thousand words, right? So let's try to represent the information in a concise way that will lead to other questions. We will look at the frequency of calls and incidents across time to help discern if there is a relationship between these variables.</p>\n<p>Often times restructuring of data is required to perform new operations like plotting. <code>ggplot2</code> is amenable to \"long format\" data rather than \"wide format\" data. To plot time series data in <code>ggplot2</code> we would like one column to represent the dates, one column to represent the counts, and one column to map each observation to either <code>n_calls</code> or <code>n_incidents</code>. This allows us to pass a single column to each <code>x</code>, <code>y</code>, and <code>color</code> argument in <code>ggplot()</code>. We want the <code>key</code> column in <code>gather()</code> to define the columns <code>n_incidents</code> and <code>n_calls</code>. The <code>value</code> column will define each of these variable's corresponding counts. By leaving the <code>Date</code> column out of <code>key</code>, the result is a long and narrow data frame with multiple rows for each  date observation.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"24"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Gather into long format using the \"Date\" column to define observations\nplot_shared_dates <- shared_dates %>%\n  gather(key = \"report\", value = \"count\", -Date)\n\n# Plot points and regression trend lines\nggplot(plot_shared_dates, aes(x = Date, y = count, color = report)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x)"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"31"},"editable":false},"cell_type":"markdown","source":"## 5. Correlation between trends\n<p>A more quantitive way to discern the relationship between 2 variables is to calculate a correlation coefficient between vectors of data. This statistic is represented between a range from -1 to +1; specifically it represents the linear dependence between two sets of data. The correlation coefficient can be interpreted as perfect negative correlation when -1, no correlation whatsoever when 0, and perfect positive correlation when +1. We will look at 2 different but related correlation coefficients in order to understand how our interpretation of statistics can greatly influence the conclusions we come to.</p>\n<p>We will first check if there is a correlation between the frequency of incidents and calls on a day-to-day basis. However, this may be too granular of a statistic - year over year daily correlations are probably only likely on big events (New Year's Eve, Halloween, Bay to Breakers). It may be helpful to take a broader view of inherent trends by summarising the data into monthly counts and calculating a correlation coefficient.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"31"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Calculate correlation coefficient between daily frequencies\ndaily_cor <- cor(shared_dates$n_incidents, shared_dates$n_calls)\nprint(daily_cor)\n\n# Summarize frequencies by month\ncorrelation_df <- shared_dates %>% \n  mutate(month = month(Date)) %>%\n  group_by(month) %>% \n  summarize(n_incidents = sum(n_incidents),\n            n_calls = sum(n_calls))\n\n# Calculate correlation coefficient between monthly frequencies\nmonthly_cor <- cor(correlation_df$n_incidents, correlation_df$n_calls)\nprint(monthly_cor)"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"38"},"editable":false},"cell_type":"markdown","source":"## 6. Filtering joins\n<p>When working with relational datasets there are situations in which it is helpful to subset information based on another set of values. Remember mutating joins? Filtering joins are a complementary type of join which allows us to keep all specific cases within a data frame while preserving the structure of the data frame itself. It will be helpful to have all the information from each police reported incident and each civilian call on their shared dates so we can calculate similar statistics from each dataset and compare results. In this case we will use <code>shared_dates</code> to subset down both the full <code>calls</code> and <code>incidents</code> data frames.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"38"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Subset calls to police by shared_dates\ncalls_shared_dates <- semi_join(calls, shared_dates, by = \"Date\")\n\n# Perform a sanity check that we are using this filtering join function appropriately\nidentical(sort(unique(shared_dates$Date)), sort(unique(calls_shared_dates$Date)))\n\n# Filter recorded incidents by shared_dates\nincidents_shared_dates <- semi_join(incidents, shared_dates, by = \"Date\")"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"45"},"editable":false},"cell_type":"markdown","source":"## 7. True crime\n<p>Back to some data viz! Now we need to see what the data look like after joining the datasets. Previously we made a scatterplot and fit a linear model to the data to see if there was a trend in the frequency of calls and the frequency of reported incidents over time. Scatterplots are a great tool to look at overall trends of continuous data. However, to see trends in categorical data, we need to visualize the ranked order of the variables to understand their levels of importance.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"45"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Create a bar chart of the number of calls for each crime\nplot_calls_freq <- calls_shared_dates %>% \n  count(Descript) %>% \n  top_n(15, n) %>% \n  ggplot(aes(x = reorder(Descript, n), y = n)) +\n  geom_bar(stat = 'identity') +\n  ylab(\"Count\") +\n  xlab(\"Crime Description\") +\n  ggtitle(\"Calls Reported Crimes\") +\n  coord_flip()\n\n# Create a bar chart of the number of reported incidents for each crime\nplot_incidents_freq <- incidents_shared_dates %>% \n  count(Descript) %>% \n  top_n(15, n)  %>% \n  ggplot(aes(x = reorder(Descript, n), y = n)) +\n  geom_bar(stat = 'identity') +\n  ylab(\"Count\") +\n  xlab(\"Crime Description\") +\n  ggtitle(\"Incidents Reported Crimes\") +\n  coord_flip()\n\n# Output the plots\nplot_calls_freq\nplot_incidents_freq"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"52"},"editable":false},"cell_type":"markdown","source":"## 8. Grand theft auto\n<p>Interesting - far and away the crime of highest incidence is \"GRAND THEFT FROM LOCKED AUTO\". This category probably captures many crimes of opportunity where unsupervised vehicles are broken into. However, there <strong>are</strong> vigilantes out there trying to prevent crime! The 12th most civilian reported crime is \"Auto Boost / Strip\"! Maybe these civilians are truly helping to prevent crime. Yet, this is probably only the case where the location of a called-in-crime is similar to the location of crime incidence. Let's check to see if the locations of the most frequent civilian reported crime and police reported crime are similar.</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"52"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Arrange the top 10 locations of called in crimes in a new variable\nlocation_calls <- calls_shared_dates %>%\n  filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\") %>% \n  count(Address) %>% \n  arrange(desc(n)) %>% \n  top_n(10, n)\n\n# Arrange the top 10 locations of reported incidents in a new variable\nlocation_incidents <- incidents_shared_dates %>%\n  filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\") %>% \n  count(Address) %>% \n  arrange(desc(n)) %>% \n  top_n(10, n)\n\n# Print the top locations of each dataset for comparison\nprint(\"Top locations for stolen vehicles reported by civilians:\")\nprint(location_calls)\n\nprint(\"Top locations for stolen vehicles reported by police:\")\nprint(location_incidents)"},{"metadata":{"deletable":false,"run_control":{"frozen":true},"tags":["context"],"dc":{"key":"59"},"editable":false},"cell_type":"markdown","source":"## 9. Density map\n<p>It appears the datasets share locations where auto crimes occur and are reported most frequently - such as on Point Lobos Avenue, Lyon Street, and Mission Street. It would be great to plot co-occurrence of these locations to visualize overlap, however we only have longitude and latitude data for police reported <code>incidents</code>. No matter, it will still be very valuable to inspect the frequency of auto crime occurrence on a map of San Francisco. This will give us immediate insight as to where auto crimes occur. Most importantly, this visualization will provide a powerful means of communication.</p>\n<p>As we ask deeper questions it becomes obvious that many details of each dataset are not standardized (such as the <code>Address</code> variable in each data frame and the lack of exact location data in <code>calls</code>) and thus require more advanced analysis. Now this is the fun part - applying your technical creativity to difficult questions. Go forth from here, check out the <a href=\"https://www.kaggle.com/san-francisco/sf-police-calls-for-service-and-incidents\">original dataset</a>, and ask some new questions!</p>"},{"metadata":{"trusted":true,"tags":["sample_code"],"dc":{"key":"59"}},"outputs":[],"cell_type":"code","execution_count":null,"source":"# Load ggmap\nlibrary(ggmap)\n\n# Read in a static map of San Francisco \nsf_map <- readRDS(\"datasets/sf_map.RDS\")\n\n# Filter grand theft auto incidents\nauto_incidents <- incidents_shared_dates %>% \n    filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\")\n\n# Overlay a density plot of auto incidents on the map\nggmap(sf_map) +\n  stat_density_2d(\n    aes(x = X, y = Y, fill = ..level..), alpha = 0.15,\n    size = 0.01, bins = 30, data = auto_incidents,\n    geom = \"polygon\")"}]}